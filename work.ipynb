{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#from eval import main\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "#import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install autokeras\n",
    "#!pip install --upgrade tensorflow==2.12 protobuf==4.21 tensorboard==2.12 tensorflow-estimator==2.12 keras ==2.12\n",
    "#!pip show tensorflow-intel \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_t(label):\n",
    "    if label == 'SUPPORTS':\n",
    "        t = 0\n",
    "    elif label == 'REFUTES':\n",
    "        t = 1\n",
    "    elif label == 'NOT_ENOUGH_INFO':\n",
    "        t = 2\n",
    "    elif label == 'DISPUTED':\n",
    "        t = 3\n",
    "    else:\n",
    "        return None\n",
    "    return t\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/evidence.json') as json_file:\n",
    "    evidence = json.load(json_file)\n",
    "\n",
    "with open('dataset/dev-claims.json') as json_file:\n",
    "    dev_claim = json.load(json_file)\n",
    "\n",
    "with open('dataset/dev-claims-baseline.json') as json_file:\n",
    "    dev_claims_base = json.load(json_file)\n",
    "\n",
    "with open('dataset/train-claims.json') as json_file:\n",
    "    train = json.load(json_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/test-claims-unlabelled.json') as json_file:\n",
    "    test = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_test = []\n",
    "amount = []\n",
    "for key in test.keys():\n",
    "    test[key]['claim_text'] = test[key]['claim_text'].lower()\n",
    "    sentence = test[key]['claim_text'].split()\n",
    "    word = [i for i in (sentence) if i not in set(stopwords.words('english'))] \n",
    "    #print(word)\n",
    "    amount += word\n",
    "    word = ' '.join(word)\n",
    "    claims_test.append(word)\n",
    "\n",
    "word_amount = list(set(amount))\n",
    "#print(len(word_amount))\n",
    "vectorizer = CountVectorizer(max_features = len(amount))\n",
    "bow_claim = vectorizer.fit_transform(claims_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w_evi = []\n",
    "for key in train.keys():\n",
    "    word = []\n",
    "    for evi in train[key]['evidences']:\n",
    "        word.append(evi[9:])\n",
    "        #print(evi)\n",
    "        #print(evi[9:])\n",
    "        #break\n",
    "    word = ' '.join(word)\n",
    "    w_evi.append(word)\n",
    "    #print(word)\n",
    "    #sentence = train[key]['claim_label'].split()\n",
    "    #word = [i for i in (sentence) if i not in set(stopwords.words('english'))] \n",
    "    #print(word)\n",
    "    #word_amount += word\n",
    "    #word = ' '.join(word)\n",
    "    #responds.append(word)\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "evidence_train = vectorizer.fit_transform(w_evi).toarray()\n",
    "\n",
    "#print(evidence_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "responds = []\n",
    "counter = 0\n",
    "w_claim = []\n",
    "word_amount = []\n",
    "label = []\n",
    "for key in train.keys():\n",
    "    \n",
    "    rep = label_t(train[key]['claim_label'])\n",
    "    if rep == None:\n",
    "        print(\"error response\")\n",
    "        break\n",
    "    #else:\n",
    "        #responds.append(rep)\n",
    "        \"\"\"\n",
    "        counter +=1\n",
    "        if rep in responds.keys():\n",
    "            responds[rep] +=1\n",
    "        else:\n",
    "            responds[rep] =1\n",
    "        \"\"\"\n",
    "    \n",
    "    train[key]['claim_text'] = train[key]['claim_text'].lower()\n",
    "    sentence = train[key]['claim_text'].split()\n",
    "    word = [i for i in (sentence) if i not in set(stopwords.words('english'))] \n",
    "    #print(word)\n",
    "    word_amount += word\n",
    "    word = ' '.join(word)\n",
    "    w_claim.append(word)\n",
    "\n",
    "    sentence = train[key]['claim_label'].split()\n",
    "    word = [i for i in (sentence) if i not in set(stopwords.words('english'))] \n",
    "    #print(word)\n",
    "    #word_amount += word\n",
    "    word = ' '.join(word)\n",
    "    responds.append(word)\n",
    "\n",
    "\n",
    "\n",
    "word_amount = list(set(word_amount))\n",
    "#print(len(word_amount))\n",
    "vectorizer = CountVectorizer(max_features = len(word_amount))\n",
    "bow_claim = vectorizer.fit_transform(w_claim).toarray()\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "responds = vectorizer.fit_transform(responds).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading BERT model.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "print(\"Done loading BERT model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1662, 691, 318, 612, 645, 5654, 2370, 326, 763, 17, 318, 257, 36156, 415, 11, 2440, 763, 17, 14587, 1682, 1037, 30020, 1104, 517, 4618, 290, 5044, 1204, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "claim_text = {}\n",
    "for key in train.keys():\n",
    "    train[key]['claim_text'] = train[key]['claim_text'].lower()\n",
    "    claim_text[key] = tokenizer(train[key]['claim_text'], padding=False, truncation=True)\n",
    "    print(claim_text[key])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['DISPUTED'],\n",
       "       ['REFUTES'],\n",
       "       ['SUPPORTS'],\n",
       "       ...,\n",
       "       ['NOT_ENOUGH_INFO'],\n",
       "       ['NOT_ENOUGH_INFO'],\n",
       "       ['SUPPORTS']], dtype='<U15')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre = np.load('test.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pre = np.load('test.npy').tolist()\n",
    "\n",
    "\n",
    "with open('test-claims-prediction.json') as json_file:\n",
    "    format = json.load(json_file)\n",
    "\n",
    "counter = 0\n",
    "for key in format.keys():\n",
    "    format[key]['claim_label'] = pre[counter][0]\n",
    "    counter +=1\n",
    "\n",
    "file_path = '1.json'\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(format, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
