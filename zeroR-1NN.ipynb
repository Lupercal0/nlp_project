{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers nlp_function\n",
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.spatial.distance import cdist\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#from nlp_function import pick_random_keys, stopwords_func, lower_processing\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Remove stopwords from claim and evidence for reducing the computational consumption'''\n",
    "def stopwords_func(stop_words, text_type, text_data):\n",
    "    if text_type == \"evidence\":\n",
    "        for i in text_data:\n",
    "            sentence = text_data[i]\n",
    "            words = sentence.split()\n",
    "            filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "            filtered_sentence = \" \".join(filtered_words)\n",
    "            text_data[i] = filtered_sentence\n",
    "    else:\n",
    "        for i in text_data.values():\n",
    "            sentence = i[\"claim_text\"]\n",
    "            words = sentence.split()\n",
    "            filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "            filtered_sentence = \" \".join(filtered_words)\n",
    "            i[\"claim_text\"] = filtered_sentence\n",
    "    return text_data\n",
    "\n",
    "'''Function for picking random keys from the dictionary after excluding the specified key(s)'''\n",
    "def pick_random_keys(dictionary, excluded_keys, num_keys):\n",
    "    available_keys = [key for key in dictionary.keys() if key not in excluded_keys]\n",
    "    random_keys = random.sample(available_keys, num_keys)\n",
    "    return random_keys\n",
    "\n",
    "'''Function for turning the text into lowercase expression'''\n",
    "def lower_processing(data, text_type):\n",
    "    if text_type == \"claim_text\":\n",
    "        for i in data:\n",
    "            data[i][text_type] = data[i][text_type].lower()\n",
    "    else:\n",
    "        for i in data:\n",
    "            data[i] = data[i].lower()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in data\n",
    "# Read in training data (claim)\n",
    "with open('dataset/train-claims.json', 'r') as tclaim_file:\n",
    "    tclaim_data = json.load(tclaim_file)\n",
    "\n",
    "# Read in development data (claim)\n",
    "with open('dataset/dev-claims.json', 'r') as dclaim_file:\n",
    "    dclaim_data = json.load(dclaim_file)\n",
    "\n",
    "# Read in test data (claim)\n",
    "with open('dataset/test-claims-unlabelled.json', 'r') as uclaim_file:\n",
    "    uclaim_data = json.load(uclaim_file)\n",
    "\n",
    "# Read in evidence data\n",
    "with open('dataset/evidence.json', 'r') as evi_file:\n",
    "    evi_data = json.load(evi_file)\n",
    "\n",
    "## Preprocessing - Lowercase operation of the case\n",
    "tclaim_data = lower_processing(tclaim_data, \"claim_text\")\n",
    "dclaim_data = lower_processing(dclaim_data, \"claim_text\")\n",
    "uclaim_data = lower_processing(uclaim_data, \"claim_text\")\n",
    "evi_data = lower_processing(evi_data, 'evidence')\n",
    "\n",
    "# ## Remove stopwords from claims and evidence (optional)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tclaim_data = stopwords_func(stop_words, \"claim\", tclaim_data)\n",
    "dclaim_data = stopwords_func(stop_words, \"claim\", dclaim_data)\n",
    "uclaim_data = stopwords_func(stop_words, \"claim\", uclaim_data)\n",
    "evi_data = stopwords_func(stop_words, \"evidence\", evi_data)\n",
    "\n",
    "## Create claim-evidence pair based on training set\n",
    "train_pairs = []\n",
    "for i in tclaim_data.values():\n",
    "    for j in i[\"evidences\"]:\n",
    "        train_pairs.append((i[\"claim_text\"], evi_data[j], 1))\n",
    "\n",
    "## insert negative sample to the training set\n",
    "for i in tclaim_data.values():\n",
    "    excluded_keys = i[\"evidences\"]\n",
    "    random_keys = pick_random_keys(evi_data, excluded_keys, len(excluded_keys))\n",
    "    for j in random_keys:\n",
    "        train_pairs.append((i[\"claim_text\"], evi_data[j], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)925a9/.gitattributes: 100%|██████████| 690/690 [00:00<00:00, 230kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 63.4kB/s]\n",
      "Downloading (…)1a515925a9/README.md: 100%|██████████| 3.99k/3.99k [00:00<00:00, 1.93MB/s]\n",
      "Downloading (…)515925a9/config.json: 100%|██████████| 550/550 [00:00<00:00, 183kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 122/122 [00:00<00:00, 60.2kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 265M/265M [00:24<00:00, 11.0MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 26.5kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 37.3kB/s]\n",
      "Downloading (…)925a9/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 750kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 450/450 [00:00<00:00, 222kB/s]\n",
      "Downloading (…)1a515925a9/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 17.5MB/s]\n",
      "Downloading (…)15925a9/modules.json: 100%|██████████| 229/229 [00:00<00:00, 114kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Obtain sentence list\n",
    "sentence_dict = {\"train\": 0, \"test\": 0}\n",
    "sentence_list = []\n",
    "for i in tclaim_data:\n",
    "    sentence_dict[\"train\"] += 1\n",
    "    sentence_list.append(tclaim_data[i][\"claim_text\"])\n",
    "for i in uclaim_data:\n",
    "    sentence_dict[\"test\"] += 1\n",
    "    sentence_list.append(uclaim_data[i][\"claim_text\"])\n",
    "\n",
    "# Load pre-trained SBERT model\n",
    "model_name = 'distilbert-base-nli-mean-tokens'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Embed sentences and obtain test set vectors\n",
    "embeddings = model.encode(sentence_list)\n",
    "train_matrix = embeddings[:sentence_dict[\"train\"]]\n",
    "test_matrix = embeddings[sentence_dict[\"train\"]:]\n",
    "\n",
    "# Capture the closest training instance (index) to the test set\n",
    "test_train_index = []\n",
    "for i in range(test_matrix.shape[0]):\n",
    "    distances = cdist(train_matrix, np.expand_dims(test_matrix[i], axis=0), metric='euclidean')\n",
    "    test_train_index.append(np.argmin(distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-R Classification \n",
    "label_list = []\n",
    "for i in tclaim_data.values():\n",
    "    label_list.append(i[\"claim_label\"])\n",
    "strings = label_list\n",
    "counter = Counter(strings)\n",
    "most_common = counter.most_common(1)\n",
    "most_frequent_string = most_common[0][0]\n",
    "frequency = most_common[0][1]\n",
    "\n",
    "# Assign label and evidence to the test set\n",
    "train_key_list = list(tclaim_data.keys())\n",
    "count = 0\n",
    "for i in uclaim_data.values():\n",
    "    i[\"claim_label\"] = most_frequent_string\n",
    "    i[\"evidences\"] = tclaim_data[train_key_list[test_train_index[count]]][\"evidences\"]\n",
    "    count += 1\n",
    "\n",
    "# Save the test set result\n",
    "file_path = 'dataset/test-claims-predictions.json'\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(uclaim_data, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
